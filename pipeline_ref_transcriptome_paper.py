##############################################################################
#
#   MRC FGU CGAT
#
#   $Id$
#
#   Copyright (C) 2017 Tom Smith
#
#   This program is free software; you can redistribute it and/or
#   modify it under the terms of the GNU General Public License
#   as published by the Free Software Foundation; either version 2
#   of the License, or (at your option) any later version.
#
#   This program is distributed in the hope that it will be useful,
#   but WITHOUT ANY WARRANTY; without even the implied warranty of
#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#   GNU General Public License for more details.
#
#   You should have received a copy of the GNU General Public License
#   along with this program; if not, write to the Free Software
#   Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
###############################################################################
"""===========================
Pipeline Ref Transcriptome Paper
===========================

:Author: Tom Smith, Ian Sudbery
:Release: $Id$
:Date: |today|
:Tags: Python


Overview
========

This pipeline performs the analysis for the manuscript reporting on
the impact of reference transcriptome choice on quantification accuracy and differnential expression analyses

Usage
=====

See :ref:`PipelineSettingUp` and :ref:`PipelineRunning` on general
information how to use CGAT pipelines.

Configuration
-------------

The pipeline requires a configured :file:`pipeline.ini` file.
CGATReport report requires a :file:`conf.py` and optionally a
:file:`cgatreport.ini` file (see :ref:`PipelineReporting`).

Default configuration files can be generated by executing:

   python <srcdir>/pipeline_ref_transcriptome_paper.py config

Input files
-----------

None required except the pipeline configuration files.

Requirements
------------

The pipeline requires the results from
:doc:`pipeline_annotations`. Set the configuration variable
:py:data:`annotations_database` and :py:data:`annotations_dir`.

On top of the default CGAT setup, the pipeline requires the following
software to be in the path:

.. Add any additional external requirements such as 3rd party software
   or R modules below:

Requirements:

* samtools >= 1.1

Pipeline output
===============

.. Describe output files of the pipeline here

Glossary
========

.. glossary::


Code
====

"""

# import standard modules
import sys
import os
import sqlite3
import pysam
import time

# import pipeline decorators
from ruffus import *
from ruffus.combinatorics import *

# CGAT code imports
import CGAT.Experiment as E
import CGAT.GTF as GTF
import CGAT.IOTools as IOTools
import CGAT.Sra as Sra
import CGAT.FastaIterator as FastaIterator

# CGATPipeline imports
import CGATPipelines.Pipeline as P
import CGATPipelines.PipelineMapping as PipelineMapping
import CGATPipelines.PipelineRnaseq as PipelineRnaseq

# Import utility function from pipeline module file
import RefTranscriptomePaper as PipelineFunctions

# load options from the config file
PARAMS = P.getParameters(
    ["%s/pipeline.ini" % os.path.splitext(__file__)[0],
     "../pipeline.ini",
     "pipeline.ini"])

# add configuration values from associated pipelines
#
# 1. pipeline_annotations: any parameters will be added with the
#    prefix "annotations_". The interface will be updated with
#    "annotations_dir" to point to the absolute path names.
PARAMS.update(P.peekParameters(
    PARAMS["annotations_dir"],
    "pipeline_annotations.py",
    on_error_raise=__name__ == "__main__",
    prefix="annotations_",
    update_interface=True))


# if necessary, update the PARAMS dictionary in any modules file.
# e.g.:
#
# import CGATPipelines.PipelineGeneset as PipelineGeneset
# PipelineGeneset.PARAMS = PARAMS
#
# Note that this is a hack and deprecated, better pass all
# parameters that are needed by a function explicitely.

# -----------------------------------------------
# Utility functions
def connect():
    '''utility function to connect to database.

    Use this method to connect to the pipeline database.
    Additional databases can be attached here as well.

    Returns an sqlite3 database handle.
    '''

    dbh = sqlite3.connect(PARAMS["database_name"])
    statement = '''ATTACH DATABASE '%s' as annotations''' % (
        PARAMS["annotations_database"])
    cc = dbh.cursor()
    cc.execute(statement)
    cc.close()

    return dbh

# ---------------------------------------------------
# Specific pipeline tasks

##############################################################################
#  Sequins Set Up- Start                                                     #
##############################################################################

@mkdir('annotations')
@originate(['annotations/sequins_concentrations_A.tsv',
            'annotations/sequins_concentrations_B.tsv'])
def downloadSequinsConcentration(outfile):
    ''' Download the Sequins concentrations from their amazon cloud '''

    mix2id = {'A': 12, 'B': 13}
    mix = os.path.basename(outfile).split('_')[-1][0]
    csv_id = mix2id[mix]

    statement = '''
    wget https://s3.amazonaws.com/sequins/mixtures/M.R.%(csv_id)i.csv
    -O %(outfile)s '''

    P.run()


sequins_raw = []
sequins_cell_lines = ['K562', 'GM12878']
for cell_line in sequins_cell_lines:
    for replicate in range(1,4):
        for suffix in ['fastq.1.gz', 'fastq.2.gz']:
            sequins_raw.append('sequins/raw/%s-%s.%s' % (
                cell_line, replicate, suffix))



@mkdir('sequins', 'sequins/raw')
@originate(sequins_raw)
def downloadSequinsData(outfile):
    ''' Download the Sequins data from their amazon cloud '''

    address_base = 'https://s3.amazonaws.com/sequins/libraries'
    outfile2filename = {
        'K562-1.fastq.1.gz': 'L.R.1.1.fq.gz',
        'K562-1.fastq.2.gz': 'L.R.1.2.fq.gz',
        'K562-2.fastq.1.gz': 'L.R.2.1.fq.gz',
        'K562-2.fastq.2.gz': 'L.R.2.2.fq.gz',
        'K562-3.fastq.1.gz': 'L.R.3.1.fq.gz',
        'K562-3.fastq.2.gz': 'L.R.3.2.fq.gz',
        'GM12878-1.fastq.1.gz': 'L.R.4.1.fq.gz',
        'GM12878-1.fastq.2.gz': 'L.R.4.2.fq.gz',
        'GM12878-2.fastq.1.gz': 'L.R.5.1.fq.gz',
        'GM12878-2.fastq.2.gz': 'L.R.5.2.fq.gz',
        'GM12878-3.fastq.1.gz': 'L.R.6.1.fq.gz',
        'GM12878-3.fastq.2.gz': 'L.R.6.2.fq.gz'}

    filename = outfile2filename[os.path.basename(outfile)]

    statement = ' wget %(address_base)s/%(filename)s -O %(outfile)s '
    P.run()


@mkdir('sequins', 'sequins/raw')
@originate(['sequins/raw/neat-A.fastq.1.gz',
            'sequins/raw/neat-B.fastq.1.gz'])
def downloadSequinsNeatData(outfile):
    ''' Download the neat Sequins data from NCBI'''

    address_base = 'ftp://ftp-trace.ncbi.nlm.nih.gov/sra/sra-instant/reads/ByExp/sra/SRX/SRX189'

    outfile2srr = {
        'neat-A.fastq.1.gz': 'SRR3743147',
        'neat-B.fastq.1.gz': 'SRR3743148'}

    srr2srx = {
        'SRR3743147': 'SRX1897294',
        'SRR3743148': 'SRX1897295'}

    outfile_base = os.path.basename(outfile)

    srr = outfile2srr[outfile_base]
    srx = srr2srx[srr]

    outfile_name = P.snip(outfile_base, '.fastq.1.gz')

    statement = '''
    wget %(address_base)s/%(srx)s/%(srr)s/%(srr)s.sra
    -O %(outfile_name)s.sra
    '''
    P.run()

    outdir = os.path.dirname(outfile)
    statement = Sra.extract(outfile_name + '.sra', outdir)
    P.run()

    statement = '''
    mv %(outdir)s/%(outfile_name)s_1.fastq.gz
    %(outdir)s/%(outfile_name)s.fastq.1.gz; checkpoint;
    mv %(outdir)s/%(outfile_name)s_2.fastq.gz
    %(outdir)s/%(outfile_name)s.fastq.2.gz'''
    P.run()

    os.unlink(outfile_name + '.sra')


@transform(downloadSequinsNeatData,
           regex("(\S+).fastq.1.gz"),
           r"\1_subset.fastq.1.gz")
def subsetNeatData(infile, outfile):
    ''' subset the neat data down to 1M reads to make counts more
    representative of a true RNA-Seq experiment '''

    infile2 = infile.replace("fastq.1.gz", "fastq.2.gz")
    outfile2 = outfile.replace("fastq.1.gz", "fastq.2.gz")
 
    statement = '''
    zcat  %(infile)s |
    awk 'NR<4000001' |
    gzip > %(outfile)s ;
    checkpoint ;
    zcat  %(infile2)s |
    awk 'NR<4000001'  |
    gzip > %(outfile2)s
    '''

    P.run()




@mkdir('annotations')
@originate('annotations/sequins_transcripts.fa')
def downloadSequinsTranscriptomeFasta(outfile):
    ''' Download the sequins in silico contig fasta'''

    statement = '''
    wget https://s3.amazonaws.com/sequins/annotations/A.R.2.fa
    -O %(outfile)s
    '''

    P.run()


@mkdir('annotations')
@originate('annotations/sequins_contig.fa')
def downloadSequinsGenomeFasta(outfile):
    ''' Download the sequence for the in silico contig'''

    statement = '''
    wget https://s3.amazonaws.com/sequins/annotations/A.R.4.fa
    -O %(outfile)s; checkpoint; 
    samtools faidx %(outfile)s
    '''

    P.run()


@mkdir('hisat.dir')
@transform(downloadSequinsGenomeFasta,
           regex("annotations/(\S+).fa"),
           r"hisat.dir/\1.1.ht2")
def buildHisatIndex(infile, outfile):
    ''' build hisat2 index for sequins contig'''
    
    outfile = P.snip(outfile, ".1.ht2")

    statement = '''
    hisat2-build %(infile)s %(outfile)s
    '''

    P.run()

@transform(downloadSequinsGenomeFasta,
           regex('annotations/sequins_contig.fa'),
           add_inputs(os.path.join(
               PARAMS['genome_dir'],
               PARAMS['genome'] + '.fa')),
           r"annotations/hg38_sequins.fa")
def buildCombinedFasta(infiles, outfile):
    '''Concatenate the human genome with the sequins in silico contig'''

    IS_genome, genome = infiles

    statement = '''
    cat %(IS_genome)s %(genome)s > %(outfile)s; 
    samtools faidx %(outfile)s
    '''

    P.run()


@mkdir('annotations')
@originate('annotations/sequins.gtf.gz')
def downloadSequinsGTF(outfile):
    ''' download the sequins GTF'''
    
    statement = '''
    wget -q -O - https://s3.amazonaws.com/sequins/annotations/A.R.1.gtf |
    gzip > %(outfile)s
    '''

    P.run()


@transform(downloadSequinsGTF,
           regex('(\S+).gtf.gz'),
           add_inputs(downloadSequinsGenomeFasta),
           r'\1.fa')
def buildSequinsReferenceTranscriptome(infiles, outfile):
    '''
    Builds a reference transcriptome from the provided GTF geneset - generates
    a fasta file containing the sequence of each feature labelled as
    "exon" in the GTF.
    --fold-at specifies the line length in the output fasta file'''

    infile, genome_file = infiles

    statement = '''
    zcat %(infile)s |
    awk '$3=="exon"'|
    cgat gff2fasta
    --is-gtf --genome-file=%(genome_file)s --fold-at=60 -v 0
    --log=%(outfile)s.log > %(outfile)s;
    checkpoint;
    samtools faidx %(outfile)s
    '''

    P.run()


@mkdir('annotations')
@originate('annotations/gencode_all.gtf.gz')
def downloadGENCODEGTF(outfile):

    gencode_gtf = "ftp://ftp.sanger.ac.uk/pub/gencode/Gencode_human/release_25/gencode.v25.annotation.gtf.gz"
    gencode_gtf_filename = os.path.basename(gencode_gtf)

    statement = '''
    wget %(gencode_gtf)s; checkpoint;
    gunzip %(gencode_gtf_filename)s -c | gzip > %(outfile)s; checkpoint;
    rm -f %(gencode_gtf_filename)s
    '''

    P.run()


@transform(downloadGENCODEGTF,
           regex('annotations/gencode_all.gtf.gz'),
           [r"annotations/gencode_gene_info.load",
            r"annotations/gencode_transcript_info.load",
            r"annotations/gencode_transcript_tag.load"])
def tabulateGENCODEGTF(infile, outfiles):
    '''
    extract the required information from the GENCODE GTF
    and load
    '''

    gene_info_out, transcript_info_out, transcript_tag_out = [
        x.replace('.load', '.tsv') for x in outfiles]

    transcript_tmp_infile = P.getTempFilename('/scratch')
    gene_tmp_infile = P.getTempFilename('/scratch')

    statement = '''
    zcat %(infile)s | awk '$3=="transcript"' > %(transcript_tmp_infile)s ;
    checkpoint;
    zcat %(infile)s | awk '$3=="gene"' > %(gene_tmp_infile)s ;'''
    P.run()

    PipelineFunctions.tabulateGENCODETranscriptGTF(
        transcript_tmp_infile, transcript_info_out, transcript_tag_out)

    PipelineFunctions.tabulateGENCODEGeneGTF(
        gene_tmp_infile, gene_info_out)

    #os.unlink(transcript_tmp_infile)
    #os.unlink(gene_tmp_infile)

    P.load(gene_info_out, outfiles[0],
           options='--add-index=gene_id')
    P.load(transcript_info_out, outfiles[1],
           options='--add-index=transcript_id,gene_id')
    P.load(transcript_tag_out, outfiles[2],
           options='--add-index=transcript_id,gene_id')


@follows(tabulateGENCODEGTF)
@originate('annotations/gencode_protein_coding_ids.tsv')
def identifyProteinCoding(outfile):
    ''' Use the transcript_info table to identify protein coding transcripts'''

    dbh = connect()

    sql_statement = '''
    SELECT DISTINCT transcript_id
    FROM gencode_transcript_info AS A
    JOIN gencode_gene_info AS B ON A.gene_id=B.gene_id
    WHERE B.gene_type=="protein_coding"
    '''

    ids = dbh.execute(sql_statement)

    with IOTools.openFile(outfile, 'w') as outf:
        outf.write("\n".join([x[0] for x in ids]))


@follows(tabulateGENCODEGTF)
@originate('annotations/gencode_basic_ids.tsv')
def identifyBasic(outfile):
    ''' Use the transcript_info table to identify GENCODE Basic
    protein coding transcripts'''

    dbh = connect()

    sql_statement = '''
    SELECT DISTINCT A.transcript_id
    FROM gencode_transcript_info AS A
    JOIN gencode_gene_info AS B ON A.gene_id=B.gene_id
    JOIN gencode_transcript_tag AS C ON A.transcript_id=c.transcript_id
    WHERE B.gene_type=="protein_coding" AND
    C.tag=="basic"
    '''

    ids = dbh.execute(sql_statement)

    with IOTools.openFile(outfile, 'w') as outf:
        outf.write("\n".join([x[0] for x in ids]))


@transform(downloadGENCODEGTF,
           regex("annotations/gencode_all.gtf"),
           add_inputs(identifyProteinCoding),
           "annotations/gencode_coding.gtf.gz")
def buildCodingGeneSet(infiles, outfile):
    '''build a geneset with only transcripts from protein coding genes.

    Retain the transcripts from the gencode_protein_coding_ids file in
    the outfile geneset.  The gene set will contain all transcripts of
    protein coding genes, including processed transcripts. The gene
    set includes UTR and CDS.

    Parameters
    ----------
    infiles : list
    infile: str
       Input filename in :term:`gtf` format

    ids_tsv: str
       Input filename in :term:`tsv` format

    outfile: str
       Output filename in :term:`gtf` format

    '''

    infile, ids_tsv = infiles

    statement = '''
    zcat %(infile)s |
    awk '$3!="gene"'
    | cgat gtf2gtf
    --method=filter
    --filter-method=transcript
    --map-tsv-file=%(ids_tsv)s
    --log=%(outfile)s.log
    | gzip
    > %(outfile)s
    '''
    P.run()


@transform(downloadGENCODEGTF,
           regex("annotations/gencode_all.gtf"),
           add_inputs(identifyBasic),
           "annotations/gencode_coding_basic.gtf.gz")
def buildBasicCodingGeneSet(infiles, outfile):
    '''build a geneset with only transcripts from GENCODE "Basic"
    protein coding genes.

    Retain the transcripts from the gencode_protein_coding_ids file in
    the outfile geneset.  The gene set will contain all transcripts of
    protein coding genes, including processed transcripts. The gene
    set includes UTR and CDS.

    Parameters
    ----------
    infiles : list
    infile: str
       Input filename in :term:`gtf` format

    ids_tsv: str
       Input filename in :term:`tsv` format

    outfile: str
       Output filename in :term:`gtf` format

    '''

    infile, ids_tsv = infiles

    statement = '''
    zcat %(infile)s |
    awk '$3!="gene"'
    | cgat gtf2gtf
    --method=filter
    --filter-method=transcript
    --map-tsv-file=%(ids_tsv)s
    --log=%(outfile)s.log
    | gzip
    > %(outfile)s
    '''
    P.run()



@merge((buildCodingGeneSet, downloadSequinsGTF),
       'annotations/gencode_coding_sequins.gtf.gz')
def combineGTFs(infiles, outfile):
    ''' combine the gencode protein coding and sequins GTFs '''
    infiles = " ".join(infiles)

    statement = '''
    zcat %(infiles)s |
    awk '$3=="exon"'|
    gzip > %(outfile)s '''

    P.run()


@merge((buildBasicCodingGeneSet, downloadSequinsGTF),
       'annotations/gencode_basic_coding_sequins.gtf.gz')
def combineBasicGTFs(infiles, outfile):
    ''' combine the gencode protein coding and sequins GTFs '''
    infiles = " ".join(infiles)

    statement = '''
    zcat %(infiles)s |
    awk '$3=="exon"'|
    gzip > %(outfile)s '''

    P.run()



@transform(combineGTFs,
           regex('(\S+).gtf.gz'),
           add_inputs(buildCombinedFasta),
           r'\1.fa')
def buildReferenceTranscriptome(infiles, outfile):
    '''
    Builds a reference transcriptome from the provided GTF geneset - generates
    a fasta file containing the sequence of each feature labelled as
    "exon" in the GTF.
    --fold-at specifies the line length in the output fasta file

    Parameters
    ----------
    infile: str
        path to the GTF file containing transcript and gene level annotations
    genome_dir: str
        :term: `PARAMS` the directory of the reference genome
    genome: str
        :term: `PARAMS` the filename of the reference genome (without .fa)
    outfile: str
        path to output file
    '''

    infile, genome_file = infiles

    statement = '''
    zcat %(infile)s |
    awk '$3=="exon"'|
    cgat gff2fasta
    --is-gtf --genome-file=%(genome_file)s --fold-at=60 -v 0
    --log=%(outfile)s.log > %(outfile)s;
    checkpoint;
    samtools faidx %(outfile)s
    '''

    P.run()


@transform(combineBasicGTFs,
           regex('(\S+).gtf.gz'),
           add_inputs(buildCombinedFasta),
           r'\1.fa')
def buildBasicReferenceTranscriptome(infiles, outfile):
    '''
    Builds a reference transcriptome from the provided GTF geneset - generates
    a fasta file containing the sequence of each feature labelled as
    "exon" in the GTF.
    --fold-at specifies the line length in the output fasta file

    Parameters
    ----------
    infile: str
        path to the GTF file containing transcript and gene level annotations
    genome_dir: str
        :term: `PARAMS` the directory of the reference genome
    genome: str
        :term: `PARAMS` the filename of the reference genome (without .fa)
    outfile: str
        path to output file
    '''

    infile, genome_file = infiles

    statement = '''
    zcat %(infile)s |
    awk '$3=="exon"'|
    cgat gff2fasta
    --is-gtf --genome-file=%(genome_file)s --fold-at=60 -v 0
    --log=%(outfile)s.log > %(outfile)s;
    checkpoint;
    samtools faidx %(outfile)s
    '''

    P.run()


@transform(combineGTFs,
           regex("(\S+).gtf.gz$"),
           r"\1.junctions")
def buildReferenceJunctions(infile, outfile):
    '''build file with splice junctions from gtf file.

    Identify the splice junctions from a gene set :term:`gtf`
    file. A junctions file is a better option than supplying a GTF
    file, as parsing the latter often fails. See:

    http://seqanswers.com/forums/showthread.php?t=7563

    Parameters
    ----------
    infile : str
       Input filename in :term:`gtf` format
    outfile: str
       Output filename

    '''

    outf = IOTools.openFile(outfile, "w")
    njunctions = 0
    for gffs in GTF.transcript_iterator(
            GTF.iterator(IOTools.openFile(infile, "r"))):

        gffs.sort(key=lambda x: x.start)
        end = gffs[0].end
        for gff in gffs[1:]:
            # subtract one: these are not open/closed coordinates but
            # the 0-based coordinates
            # of first and last residue that are to be kept (i.e., within the
            # exon).
            outf.write("%s\t%i\t%i\t%s\n" %
                       (gff.contig, end - 1, gff.start, gff.strand))
            end = gff.end
            njunctions += 1

    outf.close()

    if njunctions == 0:
        E.warn('no junctions found in gene set')
        return
    else:
        E.info('found %i junctions before removing duplicates' % njunctions)

    # make unique
    statement = '''mv %(outfile)s %(outfile)s.tmp;
                   cat < %(outfile)s.tmp | sort | uniq > %(outfile)s;
                   rm -f %(outfile)s.tmp; '''
    P.run()


@follows(
    downloadSequinsConcentration,
    downloadSequinsData,
    downloadSequinsNeatData,
    subsetNeatData,
    buildSequinsReferenceTranscriptome,
    buildCombinedFasta,
    downloadSequinsGTF,
    downloadGENCODEGTF,
    tabulateGENCODEGTF,
    buildReferenceJunctions)
def setUpSequins():
    pass

##############################################################################
#  Sequins Set Up - End                                                      #
##############################################################################

##############################################################################
#  Sequins Extract - Start                                                   #
##############################################################################
@transform(buildReferenceTranscriptome,
           regex("(\S+).fa"),
           r"\1.kallisto_21.index")
def buildKallistoIndex21(infile, outfile):
    '''
    Builds a kallisto index for the reference transcriptome

    Parameters
    ----------
    infile: str
       path to reference transcriptome - fasta file containing transcript
       sequences
    outfile: str
       path to output file

    '''

    job_memory = "12G"

    # harcoded kmer size to 21

    statement = '''
    kallisto index -i %(outfile)s -k 21 %(infile)s
    '''

    P.run()


@mkdir("quant.dir/kallisto/full")
@transform((downloadSequinsData,
            downloadSequinsNeatData),
           regex("sequins/raw/(\S+).fastq.1.gz"),
           add_inputs(buildKallistoIndex21),
           r"quant.dir/kallisto/full/\1/abundance.h5")
def pseudoalignWithKallisto(infiles, outfile):
    ''' pseudoalign with kallisto '''

    infile, index = infiles

    job_threads = PARAMS['alignment_free_threads']
    job_memory = "6G"

    kallisto_options = PARAMS["kallisto_options"]
    kallisto_bootstrap = PARAMS["alignment_free_bootstrap"]

    m = PipelineMapping.Kallisto(pseudobam=1, readable_suffix='.tsv')
    statement = m.build((infile,), outfile)

    P.run()


@mkdir('sequins/extracted')
@transform(pseudoalignWithKallisto,
           regex("quant.dir/kallisto/full/(\S+)/abundance.h5"),
           r'sequins/extracted/\1.fastq.1.gz')
def extractSequinsReads(infile, outfile):
    ''' extract the sequins reads from the alignment'''

    outfile2 = outfile.replace('fastq.1.gz', 'fastq.2.gz')

    samfile = pysam.Samfile(infile + ".bam", 'rb')
    tmp_outfile = outfile.replace('.fastq.1.gz', '_tmp.bam')
    outsam = pysam.Samfile(tmp_outfile, 'wb', template=samfile)

    for read in samfile:
        if not read.is_proper_pair:
            continue
        if read.mate_is_unmapped:
            continue
        if read.is_secondary:
            continue
        if read.reference_name.startswith('ENST'):
            continue
        else:
            outsam.write(read)

    outsam.close()

    outfile_base = outfile.replace('.fastq.1.gz', '')

    statement = '''
    cat %(tmp_outfile)s |
    cgat bam2fastq
    --output-filename-pattern=%(outfile_base)s.fastq.%%s.gz
    --log=%(outfile)s.log
    '''

    P.run()

    os.unlink(tmp_outfile)


@follows(extractSequinsReads)
def extractSequins():
    pass

##############################################################################
#  Sequins Extract - End                                                     #
##############################################################################


##############################################################################
#  Sequins Quantify - Start                                                  #
##############################################################################

@transform(buildSequinsReferenceTranscriptome,
           regex("(\S+).fa"),
           r"\1.kallisto.index")
def buildKallistoIndex(infile, outfile):
    '''
    Builds a kallisto index for the reference transcriptome

    Parameters
    ----------
    infile: str
       path to reference transcriptome - fasta file containing transcript
       sequences
    alignment_free_kmer: int
       :term: `PARAMS` kmer size for Kallisto.  Default is 31.
       Kallisto will ignores transcripts shorter than this.
    outfile: str
       path to output file

    '''

    job_memory = "12G"

    statement = '''
    kallisto index -i %(outfile)s -k %(alignment_free_kmer)s %(infile)s
    '''

    P.run()


@transform(buildSequinsReferenceTranscriptome,
           regex("(\S+).fa"),
           r"\1.salmon.index")
def buildSalmonIndex(infile, outfile):
    '''
    Builds a salmon index for the reference transriptome
    Parameters
    ----------
    infile: str
       path to reference transcriptome - fasta file containing transcript
       sequences
    alignment_free_kmer: int
       :term: `PARAMS` kmer size for sailfish.  Default is 31.
       Salmon will ignores transcripts shorter than this.
    salmon_index_options: str
       :term: `PARAMS` string to append to the salmon index command to
       provide specific options e.g. --force --threads N
    outfile: str
       path to output file
    '''

    job_memory = "2G"

    # need to remove the index directory (if it exists) as ruffus uses
    # the directory timestamp which wont change even when re-creating
    # the index files
    statement = '''
    rm -rf %(outfile)s; checkpoint;
    salmon index %(salmon_index_options)s -t %(infile)s -i %(outfile)s
    -k %(alignment_free_kmer)s
    '''

    P.run()


@transform(buildSequinsReferenceTranscriptome,
           regex("(\S+).fa"),
           r"\1.sailfish.index")
def buildSailfishIndex(infile, outfile):
    '''
    Builds a sailfish index for the reference transcriptome
    Parameters
    ----------
    infile: str
       path to reference transcriptome - fasta file containing transcript
       sequences
    alignment_free_kmer: int
       :term: `PARAMS` kmer size for sailfish.  Default is 31.
       Sailfish will ignores transcripts shorter than this.
    sailfish_index_options: str
       :term: `PARAMS` string to append to the sailfish index command to
       provide specific options e.g. --force --threads N
    outfile: str
       path to output file
    '''
    # sailfish indexing is more memory intensive than Salmon/Kallisto
    job_memory = "6G"

    # need to remove the index directory (if it exists) as ruffus uses
    # the directory timestamp which wont change even when re-creating
    # the index files
    statement = '''
    rm -rf %(outfile)s; checkpoint;
    sailfish index --transcripts=%(infile)s --out=%(outfile)s
    --kmerSize=%(alignment_free_kmer)s
    %(sailfish_index_options)s
    '''

    P.run()


@follows(buildSailfishIndex,
         buildSalmonIndex,
         buildKallistoIndex,
)
def buildIndexes():
    pass


@transform(combineGTFs,
           regex('(\S+).gtf.gz'),
           r'\1_transcript2gene_map.tsv')
def getTranscript2GeneMap(infile, outfile):
    PipelineFunctions.getTranscript2GeneMap(infile, outfile)


@mkdir(("kallisto.dir", "kallisto.dir/extracted/", "kallisto.dir/raw/"))
@transform((downloadSequinsData,
            downloadSequinsNeatData,
            extractSequinsReads),
           regex('sequins/(\S+)/(\S+).fastq.1.gz'),
           add_inputs(buildKallistoIndex, getTranscript2GeneMap),
           [r'kallisto.dir/\1/\2/transcripts.tsv.gz',
            r'kallisto.dir/\1/\2/genes.tsv.gz'])
def runKallisto(infiles, outfiles):

    '''
    Computes read counts across transcripts and genes based on a fastq
    file and an indexed transcriptome using Kallisto.

    Runs the kallisto "quant" function across transcripts with the specified
    options.  Read counts across genes are counted as the total in all
    transcripts of that gene (based on the getTranscript2GeneMap table)

    Parameters
    ----------
    infiles: list
        list with three components
        0 - string - path to fastq file to quantify using Kallisto
        1 - string - path to Kallisto index file
        2 - string - path totable mapping transcripts to genes

    kallisto_threads: int
       :term: `PARAMS` the number of threads for Kallisto
    kallisto_memory: str
       :term: `PARAMS` the job memory for Kallisto
    kallisto_options: str
       :term: `PARAMS` string to append to the Kallisto quant command to
       provide specific
       options, see https://pachterlab.github.io/kallisto/manual
    kallisto_bootstrap: int
       :term: `PARAMS` number of bootstrap samples to run.
       Note, you need to bootstrap for differential expression with sleuth
       if there are no technical replicates. If you only need point estimates,
       set to 1.  Note that bootstrap must be set to at least 1
    kallisto_fragment_length: int
       :term: `PARAMS` Fragment length for Kallisto, required for single end
       reads only
    kallisto_fragment_sd: int
       :term: `PARAMS` Fragment length standard deviation for Kallisto,
       required for single end reads only.
    outfiles: list
       paths to output files for transcripts and genes
    '''

    fastqfile, index, transcript2geneMap = infiles

    transcript_outfile, gene_outfile = outfiles
    Quantifier = PipelineRnaseq.KallistoQuantifier(
        infile=fastqfile,
        transcript_outfile=transcript_outfile,
        gene_outfile=gene_outfile,
        annotations=index,
        job_threads=PARAMS["alignment_free_threads"],
        job_memory=PARAMS["kallisto_memory"],
        options=PARAMS["kallisto_options"],
        bootstrap=PARAMS["alignment_free_bootstrap"],
        fragment_length=PARAMS["kallisto_fragment_length"],
        fragment_sd=PARAMS["kallisto_fragment_sd"],
        transcript2geneMap=transcript2geneMap)

    Quantifier.run_all()


@mkdir(("sailfish.dir", "sailfish.dir/extracted/", "sailfish.dir/raw/"))
@transform((downloadSequinsData,
            downloadSequinsNeatData,
            extractSequinsReads),
           regex('sequins/(\S+)/(\S+).fastq.1.gz'),
           add_inputs(buildSailfishIndex, getTranscript2GeneMap),
           [r'sailfish.dir/\1/\2/transcripts.tsv.gz',
            r'sailfish.dir/\1/\2/genes.tsv.gz'])
def runSailfish(infiles, outfiles):
    '''
    Computes read counts across transcripts and genes based on a fastq
    file and an indexed transcriptome using Sailfish.

    Runs the sailfish "quant" function across transcripts with the specified
    options.  Read counts across genes are counted as the total in all
    transcripts of that gene (based on the getTranscript2GeneMap table)

    Parameters
    ----------
    infiles: list
        list with three components
        0 - list of strings - paths to fastq files to merge then quantify
        across using sailfish
        1 - string - path to sailfish index file
        2 - string - path to table mapping transcripts to genes

    sailfish_threads: int
       :term: `PARAMS` the number of threads for sailfish
    sailfish_memory: str
       :term: `PARAMS` the job memory for sailfish
    sailfish_options: str
       :term: `PARAMS` string to append to the sailfish quant command to
       provide specific
       options, see http://sailfish.readthedocs.io/en/master/index.html
    sailfish_bootstrap: int
       :term: `PARAMS` number of bootstrap samples to run.
       Note, you need to bootstrap for differential expression with sleuth
       if there are no technical replicates. If you only need point estimates,
       set to 1.
    sailfish_libtype: str
       :term: `PARAMS` sailfish library type
       http://sailfish.readthedocs.io/en/master/library_type.html#fraglibtype
    outfiles: list
       paths to output files for transcripts and genes
    '''

    fastqfile, index, transcript2geneMap = infiles

    transcript_outfile, gene_outfile = outfiles
    Quantifier = PipelineRnaseq.SailfishQuantifier(
        infile=fastqfile,
        transcript_outfile=transcript_outfile,
        gene_outfile=gene_outfile,
        annotations=index,
        job_threads=PARAMS["alignment_free_threads"],
        job_memory=PARAMS["sailfish_memory"],
        options=PARAMS["sailfish_options"],
        bootstrap=PARAMS["alignment_free_bootstrap"],
        libtype=PARAMS['sailfish_libtype'],
        transcript2geneMap=transcript2geneMap)

    Quantifier.run_all()


@mkdir(("salmon.dir", "salmon.dir/extracted/", "salmon.dir/raw/"))
@transform((downloadSequinsData,
            downloadSequinsNeatData,
            extractSequinsReads),
           regex('sequins/(\S+)/(\S+).fastq.1.gz'),
           add_inputs(buildSalmonIndex, getTranscript2GeneMap),
           [r'salmon.dir/\1/\2/transcripts.tsv.gz',
            r'salmon.dir/\1/\2/genes.tsv.gz'])
def runSalmon(infiles, outfiles):
    '''
    Computes read counts across transcripts and genes based on a fastq
    file and an indexed transcriptome using Salmon.

    Runs the salmon "quant" function across transcripts with the specified
    options.  Read counts across genes are counted as the total in all
    transcripts of that gene (based on the getTranscript2GeneMap table)

    Parameters
    ----------
    infiles: list
        list with three components
        0 - list of strings - paths to fastq files to merge then quantify
        across using sailfish
        1 - string - path to sailfish index file
        2 - string - path to table mapping transcripts to genes

    salmon_threads: int
       :term: `PARAMS` the number of threads for salmon
    salmon_memory: str
       :term: `PARAMS` the job memory for salmon
    salmon_options: str
       :term: `PARAMS` string to append to the salmon quant command to
       provide specific
       options, see http://sailfish.readthedocs.io/en/master/salmon.html
    salmon_bootstrap: int
       :term: `PARAMS` number of bootstrap samples to run.
       Note, you need to bootstrap for differential expression with sleuth
       if there are no technical replicates. If you only need point estimates,
       set to 1.
    salmon_libtype: str
       :term: `PARAMS` salmon library type
       as for sailfish - use
       http://sailfish.readthedocs.io/en/master/library_type.html#fraglibtype
    outfiles: list
       paths to output files for transcripts and genes
    '''

    fastqfile, index, transcript2geneMap = infiles

    transcript_outfile, gene_outfile = outfiles
    Quantifier = PipelineRnaseq.SalmonQuantifier(
        infile=fastqfile,
        transcript_outfile=transcript_outfile,
        gene_outfile=gene_outfile,
        annotations=index,
        job_threads=PARAMS["alignment_free_threads"],
        job_memory=PARAMS["salmon_memory"],
        options=PARAMS["salmon_options"],
        bootstrap=PARAMS["alignment_free_bootstrap"],
        libtype=PARAMS['salmon_libtype'],
        kmer=PARAMS['alignment_free_kmer'],
        transcript2geneMap=transcript2geneMap)

    Quantifier.run_all()


@follows(buildIndexes,
         runSailfish,
         runSalmon,
         runKallisto)
def quantifySequins():
    pass


##############################################################################
#  Sequins Quantify - End                                                    #
##############################################################################

##############################################################################
#  Sequins Add Models - Start                                                #
##############################################################################

add_models_gtfs = []

for add_type in ["skip_exons", "incomplete", "3prime"]:
    for fraction in P.asList(PARAMS['%s_fractions' % add_type]):
        for iteration in range(0, PARAMS['%s_iterations' % add_type]):
            add_models_gtfs.append(
                "sequins/add_models/%s/transcripts_%s_%s.gtf.gz" % (
                    add_type, fraction, iteration))

@mkdir('sequins/add_models/skip_exons',
       'sequins/add_models/incomplete',
       'sequins/add_models/3prime')
@originate(add_models_gtfs)
def buildAddModels(outfile):
    ''' build a set of reference transcriptomes with additional
    transcripts with skipped exons, incomplete transcripts and transcripts with alternative 3' ends '''

    # how to avoid hardcoding this?
    infile = 'annotations/sequins.gtf.gz'

    fraction = os.path.basename(outfile).split("_")[1]
    method = os.path.basename(os.path.dirname(outfile))

    if method == '3prime':
        additional_options = '--min-3prime %s --max-3prime %s' % (
            PARAMS['3prime_min'], PARAMS['3prime_max'])
    else:
        additional_options = ""

    statement = '''
    cgat add_transcripts_gtf
    --infile-geneset-gtf=%(infile)s
    --outfile-geneset-tsv=%(outfile)s.tsv
    --method=%(method)s
    --fraction=%(fraction)s
    -L %(outfile)s.log
    %(additional_options)s |
    gzip > %(outfile)s
    '''
    
    P.run()


@transform(buildAddModels,
           regex("(\S+).gtf.gz"),
           r"\1.junctions")
def buildJunctionsAddModels(infile, outfile):
    '''build file with splice junctions from gtf file.

    Identify the splice junctions from a gene set :term:`gtf`
    file. A junctions file is a better option than supplying a GTF
    file, as parsing the latter often fails. See:

    http://seqanswers.com/forums/showthread.php?t=7563

    Parameters
    ----------
    infile : str
       Input filename in :term:`gtf` format
    outfile: str
       Output filename

    '''
    PipelineFunctions.buildJunctions(infile, outfile)


@transform(buildAddModels,
           regex('(\S+).gtf.gz'),
           add_inputs(downloadSequinsGenomeFasta),
           r'\1.fa')
def buildAddModelsTranscriptome(infiles, outfile):
    '''
    Builds a reference transcriptome from the provided GTF geneset - generates
    a fasta file containing the sequence of each feature labelled as
    "exon" in the GTF.
    --fold-at specifies the line length in the output fasta file

    Parameters
    ----------
    infile: str
        path to the GTF file containing transcript and gene level annotations
    genome_dir: str
        :term: `PARAMS` the directory of the reference genome
    genome: str
        :term: `PARAMS` the filename of the reference genome (without .fa)
    outfile: str
        path to output file
    '''

    infile, genome_file = infiles

    statement = '''
    zcat %(infile)s |
    awk '$3=="exon"'|
    cgat gtf2gtf --method=sort --sort-order=gene+transcript|
    cgat gff2fasta
    --is-gtf --genome-file=%(genome_file)s --fold-at=60 -v 0
    --log=%(outfile)s.log > %(outfile)s;
    checkpoint;
    samtools faidx %(outfile)s
    '''

    P.run()

# need to count Kmers for reference transcriptome too
# buildSequinsReferenceTranscriptome
@transform(buildAddModelsTranscriptome,
           regex("(\S+).fa"),
           [r"\1_transcript_kmers.tsv",
            r"\1_gene_kmers.tsv"])
def countKmers(infile, outfiles):
    ''' count the number of unique and non-unique kmers
    per transcript and per gene '''

    job_memory = '4G'

    transcript_outfile, gene_outfile = outfiles

    statement = '''
    cgat fasta2unique_kmers
    --input-fasta=%(infile)s
    --method=transcript
    --kmer-size=%(alignment_free_kmer)s
    -L %(transcript_outfile)s.log
    > %(transcript_outfile)s '''

    P.run()

    genemap = P.getTempFilename()

    with IOTools.openFile(genemap, 'w') as outf:
        outf.write("transcript_id\tgene_id\n")
        fa = FastaIterator.FastaIterator(IOTools.openFile(infile, 'r'))
        for entry in fa:
            transcript_id = entry.title.split(" ")[0]
            gene_id = "_".join(transcript_id.split("_")[0:2])
            outf.write("%s\t%s\n" % (transcript_id, gene_id))

    statement = '''
    cgat fasta2unique_kmers
    --input-fasta=%(infile)s
    --method=gene
    --genemap=%(genemap)s
    --kmer-size=%(alignment_free_kmer)s
    -L %(gene_outfile)s.log
    > %(gene_outfile)s '''

    P.run()

    os.unlink(genemap)


@transform(buildAddModelsTranscriptome,
           regex("(\S+).fa"),
           r"\1.kallisto.index")
def buildKallistoIndexAddModels(infile, outfile):
    '''
    Builds a kallisto index for the reference transcriptome
    '''

    job_memory = "12G"

    statement = '''
    kallisto index -i %(outfile)s -k 31 %(infile)s
    '''

    P.run()


@transform(buildAddModelsTranscriptome,
           regex("(\S+).fa"),
           r"\1.salmon.index")
def buildSalmonIndexAddModels(infile, outfile):
    '''
    Builds a salmon index for the reference transriptome
    Parameters
    '''

    job_memory = "2G"

    # need to remove the index directory (if it exists) as ruffus uses
    # the directory timestamp which wont change even when re-creating
    # the index files
    statement = '''
    rm -rf %(outfile)s; checkpoint;
    salmon index %(salmon_index_options)s -t %(infile)s -i %(outfile)s
    -k %(alignment_free_kmer)s
    '''

    P.run()


@transform(buildAddModelsTranscriptome,
           regex("(\S+).fa"),
           r"\1.sailfish.index")
def buildSailfishIndexAddModels(infile, outfile):
    '''
    Builds a sailfish index for the reference transcriptome
    Parameters
    '''
    # sailfish indexing is more memory intensive than Salmon/Kallisto
    job_memory = "6G"

    # need to remove the index directory (if it exists) as ruffus uses
    # the directory timestamp which wont change even when re-creating
    # the index files
    statement = '''
    rm -rf %(outfile)s; checkpoint;
    sailfish index --transcripts=%(infile)s --out=%(outfile)s
    --kmerSize=%(alignment_free_kmer)s
    %(sailfish_index_options)s
    '''

    P.run()


# hardcoded fastq file. Need to decide which fastq to quantify
@mkdir(("salmon.dir",
        "salmon.dir/add_models",
        "salmon.dir/add_models/skip_exons",
        "salmon.dir/add_models/incomplete"))
@product(buildSalmonIndexAddModels,
         formatter('sequins/add_models/(?P<MODEL>\S+)/transcripts_(?P<FRACTION>\S+)_(?P<ITERATION>\S+).salmon.index'),
         subsetNeatData,
         formatter('sequins/raw/(?P<SAMPLE>\S+).fastq.1.gz'),
         add_inputs(getTranscript2GeneMap),
         [r'salmon.dir/add_models/{MODEL[0][0]}/{FRACTION[0][0]}_{ITERATION[0][0]}_{SAMPLE[1][0]}/transcripts.tsv.gz',
          r'salmon.dir/add_models/{MODEL[0][0]}/{FRACTION[0][0]}_{ITERATION[0][0]}_{SAMPLE[1][0]}/genes.tsv.gz',])
def runSalmonAddModels(infiles, outfiles):
    '''
    Computes read counts across transcripts and genes based on a fastq
    file and an indexed transcriptome using Salmon.

    Runs the salmon "quant" function across transcripts with the specified
    options.  Read counts across genes are counted as the total in all
    transcripts of that gene (based on the getTranscript2GeneMap table)
    '''

    infiles, transcript2geneMap = infiles
    index, fastqfile = infiles
    
    transcript_outfile, gene_outfile = outfiles
    Quantifier = PipelineRnaseq.SalmonQuantifier(
        infile=fastqfile,
        transcript_outfile=transcript_outfile,
        gene_outfile=gene_outfile,
        annotations=index,
        job_threads=PARAMS["alignment_free_threads"],
        job_memory=PARAMS["salmon_memory"],
        options=PARAMS["salmon_options"],
        bootstrap=PARAMS["alignment_free_bootstrap"],
        libtype=PARAMS['salmon_libtype'],
        kmer=PARAMS['alignment_free_kmer'],
        transcript2geneMap=transcript2geneMap)

    Quantifier.run_all()


# hardcoded fastq file. Need to decide which fastq to quantify
@mkdir(("kallisto.dir",
        "kallisto.dir/add_models",
        "kallisto.dir/add_models/skip_exons",
        "salmon.dir/add_models/incomplete"))
@product(buildKallistoIndexAddModels,
         formatter('sequins/add_models/(?P<MODEL>\S+)/transcripts_(?P<FRACTION>\S+)_(?P<ITERATION>\S+).kallisto.index'),
         subsetNeatData,
         formatter('sequins/raw/(?P<SAMPLE>\S+).fastq.1.gz'),
         add_inputs(getTranscript2GeneMap),
         [r'kallisto.dir/add_models/{MODEL[0][0]}/{FRACTION[0][0]}_{ITERATION[0][0]}_{SAMPLE[1][0]}/transcripts.tsv.gz',
          r'kallisto.dir/add_models/{MODEL[0][0]}/{FRACTION[0][0]}_{ITERATION[0][0]}_{SAMPLE[1][0]}/genes.tsv.gz',])
def runKallistoAddModels(infiles, outfiles):

    '''
    Computes read counts across transcripts and genes based on a fastq
    file and an indexed transcriptome using Kallisto.

    Runs the kallisto "quant" function across transcripts with the specified
    options.  Read counts across genes are counted as the total in all
    transcripts of that gene (based on the getTranscript2GeneMap table)
    '''

    infiles, transcript2geneMap = infiles
    index, fastqfile = infiles

    transcript_outfile, gene_outfile = outfiles
    Quantifier = PipelineRnaseq.KallistoQuantifier(
        infile=fastqfile,
        transcript_outfile=transcript_outfile,
        gene_outfile=gene_outfile,
        annotations=index,
        job_threads=PARAMS["alignment_free_threads"],
        job_memory=PARAMS["kallisto_memory"],
        options=PARAMS["kallisto_options"],
        bootstrap=PARAMS["alignment_free_bootstrap"],
        fragment_length=PARAMS["kallisto_fragment_length"],
        fragment_sd=PARAMS["kallisto_fragment_sd"],
        transcript2geneMap=transcript2geneMap)

    Quantifier.run_all()


# hardcoded fastq file. Need to decide which fastq to quantify
@mkdir(("sailfish.dir",
        "sailfish.dir/add_models",
        "sailfish.dir/add_models/skip_exons",
        "salmon.dir/add_models/incomplete"))
@product(buildSailfishIndexAddModels,
         formatter('sequins/add_models/(?P<MODEL>\S+)/transcripts_(?P<FRACTION>\S+)_(?P<ITERATION>\S+).sailfish.index'),
         subsetNeatData,
         formatter('sequins/raw/(?P<SAMPLE>\S+).fastq.1.gz'),
         add_inputs(getTranscript2GeneMap),
         [r'sailfish.dir/add_models/{MODEL[0][0]}/{FRACTION[0][0]}_{ITERATION[0][0]}_{SAMPLE[1][0]}/transcripts.tsv.gz',
          r'sailfish.dir/add_models/{MODEL[0][0]}/{FRACTION[0][0]}_{ITERATION[0][0]}_{SAMPLE[1][0]}/genes.tsv.gz',])
def runSailfishAddModels(infiles, outfiles):
    '''
    Computes read counts across transcripts and genes based on a fastq
    file and an indexed transcriptome using Sailfish.

    Runs the sailfish "quant" function across transcripts with the specified
    options.  Read counts across genes are counted as the total in all
    transcripts of that gene (based on the getTranscript2GeneMap table)
    '''

    infiles, transcript2geneMap = infiles
    index, fastqfile = infiles

    transcript_outfile, gene_outfile = outfiles
    Quantifier = PipelineRnaseq.SailfishQuantifier(
        infile=fastqfile,
        transcript_outfile=transcript_outfile,
        gene_outfile=gene_outfile,
        annotations=index,
        job_threads=PARAMS["alignment_free_threads"],
        job_memory=PARAMS["sailfish_memory"],
        options=PARAMS["sailfish_options"],
        bootstrap=PARAMS["alignment_free_bootstrap"],
        libtype=PARAMS['sailfish_libtype'],
        transcript2geneMap=transcript2geneMap)

    Quantifier.run_all()


@follows(buildHisatIndex)
@mkdir(("featurecounts.dir",
        "featurecounts.dir/add_models",
        "featurecounts.dir/add_models/skip_exons"))
@transform(buildJunctionsAddModels,
           regex('sequins/add_models/skip_exons/transcripts_(\S+)_(\d+).junctions'),
           add_inputs('sequins/extracted/GM12878-1.fastq.1.gz',
                      r'sequins/add_models/skip_exons/transcripts_\1_\2.gtf.gz',
                      buildHisatIndex,
                      getTranscript2GeneMap),
           [r'featurecounts.dir/add_models/skip_exons/\1_\2/transcripts.tsv.gz',
            r'featurecounts.dir/add_models/skip_exons/\1_\2/genes.tsv.gz'])
def runFeatureCountsAddModels(infiles, outfiles):
    ''' 
    First align with hisat2 and then quantify with FeatureCounts
    '''

    junctions, infile, annotations, sequins_genome_index, transcript_map = infiles


    ### align with hisat ###
    job_threads = PARAMS["hisat_threads"]
    job_memory = PARAMS["hisat_memory"]

    tmp_outfile = P.getTempFilename()

    hisat_index_dir = os.path.dirname(sequins_genome_index)
    genome = P.snip(os.path.basename(sequins_genome_index), ".1.ht2")

    m = PipelineMapping.Hisat(
        executable='hisat2',
        strip_sequence=0,
        stranded=PARAMS["hisat_strandedness"])

    statement = m.build((infile,), tmp_outfile)

    P.run()

    ### quantify with featureCounts ###
    transcript_outfile, gene_outfile = outfiles

    Quantifier = PipelineRnaseq.FeatureCountsQuantifier(
        infile=tmp_outfile,
        transcript_outfile=transcript_outfile,
        gene_outfile=gene_outfile,
        job_threads=PARAMS['featurecounts_threads'],
        strand=PARAMS['featurecounts_strand'],
        options=PARAMS['featurecounts_options'],
        annotations=annotations)

    Quantifier.run_all()

    os.unlink(tmp_outfile)


@follows(buildSailfishIndexAddModels,
         buildSalmonIndexAddModels,
         buildKallistoIndexAddModels)
def buildAlignmentFreeIndexesAddModels():
    pass


@follows(buildAlignmentFreeIndexesAddModels,
         runSalmonAddModels,
         runKallistoAddModels,
         runSailfishAddModels,
         countKmers)
def addModels():
    pass

##############################################################################
#  Sequins Skip Exons - End                                                  #
##############################################################################


##############################################################################
# Generic pipeline tasks                                                     #
##############################################################################

@follows(setUpSequins,
         extractSequinsReads,
         quantifySequins,
         addModels)
def sequins():
    pass


@follows(sequins)
def full():
    pass


@follows(mkdir("report"))
def build_report():
    '''build report from scratch.

    Any existing report will be overwritten.
    '''

    E.info("starting report build process from scratch")
    P.run_report(clean=True)


@follows(mkdir("report"))
def update_report():
    '''update report.

    This will update a report with any changes inside the report
    document or code. Note that updates to the data will not cause
    relevant sections to be updated. Use the cgatreport-clean utility
    first.
    '''

    E.info("updating report")
    P.run_report(clean=False)


@follows(update_report)
def publish_report():
    '''publish report in the CGAT downloads directory.'''

    E.info("publishing report")
    P.publish_report()

if __name__ == "__main__":
    sys.exit(P.main(sys.argv))
